{"paragraphs":[{"text":"%cassandra\n\n// Step 1: Create table\n\nCREATE KEYSPACE IF NOT EXISTS spark_demo WITH REPLICATION = { 'class': 'NetworkTopologyStrategy', 'AWS_VPC_US_EAST_1': 1 };\n\nCREATE TABLE IF NOT EXISTS spark_demo.test(\n        key int,\n        letter text,\n        value text,\n        PRIMARY KEY(key, letter)\n    );\n\nINSERT INTO spark_demo.test (key, letter, value) VALUES (1, 'a', 'test 1');\n\nSELECT * FROM spark_demo.test;","dateUpdated":"2017-04-05T02:12:46+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1491349441073_-340172586","id":"20170404-234401_1165720010","dateCreated":"2017-04-04T11:44:01+0000","dateStarted":"2017-04-05T02:12:46+0000","dateFinished":"2017-04-05T02:12:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:180","errorMessage":""},{"text":"%dep\n\nz.load(\"/opt/zeppelin/interpreter/spark/spark-cassandra-connector-assembly-1.6.2.jar\")","dateUpdated":"2017-04-05T02:18:36+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1491349421666_1677804747","id":"20160301-221525_94398887","dateCreated":"2017-04-04T11:43:41+0000","dateStarted":"2017-04-05T02:18:36+0000","dateFinished":"2017-04-05T02:18:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:181","errorMessage":"","focus":true},{"text":"%spark \n\nimport com.datastax.spark.connector._\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.SparkContext._\n\n// STEP 2: Populate data\n\nvar testData : List[(Int, String, String)] = List()\nvar i : Int = 0;\nval letters = Array(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\")\nval r = scala.util.Random\n\nfor( i <- 1 to 10000) {\n    for( j <- 0 to r.nextInt(10)) {\n        testData = testData:+((i, letters(j), \"test value \" + i + letters(j)))\n    }\n}\n\nval testDataRdd = sc.parallelize(testData)\ntestDataRdd.saveToCassandra(\"spark_demo\", \"test\", SomeColumns(\"key\", \"letter\", \"value\"))\n","dateUpdated":"2017-04-05T02:18:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1491349421667_1677419998","id":"20170404-060534_753357302","dateCreated":"2017-04-04T11:43:41+0000","dateStarted":"2017-04-05T02:18:49+0000","dateFinished":"2017-04-05T02:20:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:182","errorMessage":"","focus":true},{"text":"%cassandra\n\nSELECT * FROM spark_demo.test LIMIT 5;","dateUpdated":"2017-04-05T02:20:25+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1491351685487_1818253943","id":"20170405-002125_1276525943","dateCreated":"2017-04-05T12:21:25+0000","dateStarted":"2017-04-05T02:20:25+0000","dateFinished":"2017-04-05T02:20:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:183","errorMessage":"","focus":true},{"text":"%spark\n\n// Step 3a: Filtering using Spark\n\nimport com.datastax.spark.connector._\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.SparkContext._\n\nval letterCountRdd = sc.cassandraTable(\"spark_demo\",\"test\")\n    .filter(r => r.getInt(\"key\").equals(1))\n    .map(((x: CassandraRow) => x.getString(\"letter\")))\n    .map(letter => (letter, 1))\n    .reduceByKey{case (x, y) => x + y}\n    \nprintln(letterCountRdd.toDebugString)\n\nletterCountRdd.collect.foreach(println)","dateUpdated":"2017-04-05T02:20:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1491349421670_1676265751","id":"20170404-203712_1050894625","dateCreated":"2017-04-04T11:43:41+0000","dateStarted":"2017-04-05T02:20:58+0000","dateFinished":"2017-04-05T02:21:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:184","errorMessage":"","focus":true},{"text":"%spark\n\n// Step 3b: Pushing filtering down to the Cassandra node\n\nimport com.datastax.spark.connector._\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.SparkContext._\n\nval letterCountRdd = sc.cassandraTable(\"spark_demo\",\"test\")\n    .select(\"letter\").where(\"key = ?\", \"1\")\n    .map(((x: CassandraRow) => x.getString(\"letter\")))\n    .map(letter => (letter, 1))\n    .reduceByKey{case (x, y) => x + y}\n\nprintln(letterCountRdd.toDebugString)\n\nletterCountRdd.collect.foreach(println)","dateUpdated":"2017-04-05T02:46:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1491349421668_1675496254","id":"20170404-201430_598826338","dateCreated":"2017-04-04T11:43:41+0000","dateStarted":"2017-04-05T02:46:49+0000","dateFinished":"2017-04-05T02:46:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:185","errorMessage":"","focus":true},{"text":"","dateUpdated":"2017-04-04T11:43:41+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1491349421670_1676265751","id":"20170404-061828_1177051808","dateCreated":"2017-04-04T11:43:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:187"}],"name":"blog-post","id":"2CD53945C","angularObjects":{"SparkInterpreter:shared_process":[],"2BC7NAFKU:shared_process":[],"2B9G9VSP8:shared_process":[],"CassandraInterpreter:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}